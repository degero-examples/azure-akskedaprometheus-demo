# Kubernetes KEDA scaled demo application

This demo K8S app is composed of two apps (app-one, app-two) both running nginx serving custom html pages for each located in the volume-data folder.  The deployment is configurable to use on a private network with load balancer or using a nginx ingress control to access the applications. There are other configuation options covered below to showcase some features to make use of.

To not incur any expenses while testing the app it can be run locally using Docker and KIND covered below.

## Prereqs

- [Kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl/)
- [Helm](https://helm.sh/docs/intro/install/)
- Bash shell

## Configuration files

Make adjustments (can be left as default) to the below base on what you set in your /infra deployment and preference:

### values-base.yaml

- PodAffinitity/AntiAffinity (Default: true. setting appOne/Two.enableAffinity to ensure one instance of app per node)
- App Resources/Scale/ContainerImage settings (settings: appOne.*/appTwo.*)

### values-azure-(ingress/lb).yaml (setttings specific to azure deployment)

- Private Network IPs for LoadBalanced apps on VNET (lbAppOne/Two.annotations)
- Scale Query (appOne/appTwo.scaleQuery. To alter what metrics trigger pod scaling)

### values-localdev-(ingress/lb).yaml (setttings specific to local KIND deployment)

- Scale Query (appOne/appTwo.scaleQuery. To alter what metrics trigger pod scaling)

### /scripts/env.azure

This is autogenerated if you ran the /infra/deploy.sh.

If not you can copy .env.azure.sample to .env.azure adjust the names as set in your infra and run these commands to retrieve the empty values in the sample needed and write them in to the file:

```
source .env.azure
export PROMETHEUSADDRESS=$(az monitor account show -n amw-$APPNAME-$ENV -g $RESOURCE_GROUP --query "metrics.prometheusQueryEndpoint" -o tsv)
export AZURE_TENANT_ID=$(az account show --query "tenantId" -o tsv)
```

Configurable options:

- Set USE_AKS_APP_ROUTING_ADDON=true if you set enableAKSAppRoutingAddon=true in the infra deployment (this is only for use when privateNetwork.enabled=false in the infra deployment)
- Set PRIVATE_NETWORK=true to enable Private VNET when privateNetwork.enabled=true was set in infra deployment
- If you changed the appname or env in the infra deployment replace the resourcenames / ENV / APPNAME 'kedasclerapp' name

## Apply HELM chart locally

Unfortunately it's never likely to be a 1:1 of running k8s deployment in a local env to a cloud provider so there are some limitations this workload has to work around. The local implementation uses [KIND specific cloud load balancer](https://github.com/kubernetes-sigs/cloud-provider-kind) to act like a cloud provider load balancer.
Also due to the local env limitations a KIND specific nginx ingress controller manifest is used. ingress is based on subpaths of localhost; loadbalancer is port based for either app.
In a cloud hosted env it is more likely the ingress/load balancer will be use by IP/hostname per app.

Due to an issue with helm / and the kubernetes API version in KIND you may see the below warning during install which can be ignored:
"Warning: unrecognized format ..." 

## Prereqs

- [Docker](https://docs.docker.com/engine/install/)
- [KIND](https://kind.sigs.k8s.io/docs/user/quick-start/#installation) 


## Install with nginx ingress

- run sh /localdev/deploy-localdev.sh and set prompts for github token as any text and privatenetwork to false
- This will take several minutes
- browse to localhost/app-one and localhost-app-two you should see "Hello from app-one/two!"


## Install with private network load balancer

- run sh /localdev/deploy-localdev.sh and set prompts for github token as any text and privatenetwork to true
- This will take several minutes 
- In a Superuser/Administrator shell run:
docker run --network host -v /var/run/docker.sock:/var/run/docker.sock cloud-controller-manager:v0.7.0 -enable-lb-port-mapping
- run kubectl get svc and access the two apps on the external IPs listed (see below limitations non linux envs)
- check the pods are 'Ready' via kukubebectl get pod
- If using Linux: Browse to the IPs on 8080 / 8081 and you should see the contents of the volume-data subfolders served

### MacOS/Win Environment limitations

Unfortunately there are limitations to networking in these envs as noted in the [repo](https://github.com/kubernetes-sigs/cloud-provider-kind/issues/189#issuecomment-2598364846)

to access the Load balanced apps run 'docker container list'
look at the envoyproxy/envoy containers with exposed ports targeting 8080/8081 and access 127.0.0.1:<port>

### Verify workload running

kubectl get pod (verify these are ready)
if not using private network: kubectl get ingress (it should show a hostname of localhost)
If using private network: kubectl get svc (should show lb-app-one/two both on 8080)
kubectl logs -l app=keda-operator -n keda (check the logs to see the scaledobject have no issues)
kubectl port-forward -n monitoring svc/prometheus 9090:9090
Browse to localhost:9090 to view Prom metrics and look at status > target health to confirm pods are scraped (pods with name 'app-one*' or 'app-two*' only need to be scraped when using private network loadbalancer)
kubectl get scaledobject (check these are ready)

### Test scaler

- run the below twice
- for i in {1..100}; do curl http://localhost/app-one & done (or Ports if using load balancer)
- Watch pods scale up
- kubectl get pods -w
- Check pod scaling events triggered
- kubectl events --for hpa/keda-hpa-app-one-request-scaler -w
- You can also run against http://localhost/app-two and look at events for hpa/keda-hpa-app-two-request-scaler

Note that the new pod wont be able to allocate to a node due to the pod affinity / antiaffinity rules demonstration, these are here for demonstration purposes for heavy node resource use apps. You can turn this off in appOne/Two.enableAffinity=false in values-base.yaml.

## Metrics

You can view these by fowrading prometheus:
kubectl port-forward -n monitoring svc/prometheus 9090:9090

Or if you wish run a Grafana instance and view dashboards
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install grafana grafana/grafana --namespace grafana --create-namespace
follow the prompts to get the admin password and port forward the instance and login
add a prometheus data source as http://prometheus.monitoring.svc.cluster.local:9090
there are sample grafana dashboards on the project root [README.md](../README.md) to view the related metrics

## Uninstall

run /localdev/delete-localdev.sh

## Apply HELM chart in Azure

### Prereqs

- deployed /infra to Azure
- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)

Before you start:

There are a few configuration options .env.azure you can adjust to your liking and what you set in the infra deployment as mentioned in the above configuration section, comments in the file will act as guidance.

Also note when not using a private network this deployment uses a basic nginx ingress controller deployment /cluster-dependencies/general/ingress-nginx as an alternate to the AKS app routing addon. You can use this addon, however you will need a register a DNS A record pointing to the ingress IP in order for ngxin to emit metrics that are used for scaling. You can get this IP by running 'kubectl get ingress' (it takes a few minutes to provision after deploy).

If you did not deploy /infra with /infra/deploy-bicep.sh:

- cp .env.azure.sample .env.azure and alter any settings in .env.azure (if you did not deploy infra with ./infra/deploy-bicep.sh which generates it)
- Add the nginx ingress controller (if you did not set enablePrivateNetwork=true in your bicep parameters. Alternatively you can use the AKS app routing managed module by changing your .env.azure settings. It does the same thing but requires you have a hostname to point to the IP)

Using bash run

- az extension add --name aks-preview (for cluster autoscaler)
- az extension update --name aks-preview 
- apply the workload: sh /scripts/deploy-azure-workload.sh enter any value for GITHUBTOKEN
- az aks get-credentials -n <clustername> -g <resource_group>

### Check system is running correctly

Check pods ar running (they may take a few minutes to start as nodes scale up from zero):

kubectl get pods

Confirm metrics and keda scaler have no errors:

kubectl get scaledobject (should have the states 'READY' true, if not check below for errors) 

kubectl logs -n keda -l app=keda-operator
kubectl events --for hpa/keda-hpa-app-one-request-scaler -w
kubectl events --for hpa/keda-hpa-app-two-request-scaler -w
kubectl describe scaledobject (you can confirm the correct metric query in here, eg if you set privateNetwork=true you'd need to update value-azure.yaml to use the loadbalancer specific query)

See the troubleshooting sections below for more information.

### Testing scaling in Private network

If using privateNetworking, app one and two are accessible on the service.beta.kubernetes.io/azure-load-balancer-ipv4 annotations set in values-azure.yaml

```
az vm create \
  --resource-group rg-<yourappname>-dev \
  --name test-vm \
  --vnet-name vnet-<yourappname>-dev \
  --subnet virtualmachines \
  --image Ubuntu2404 \
  --size Standard_B1s \
  --admin-username azureuser \
  --generate-ssh-keys
```

```
az vm show -d \
  --resource-group rg-<yourappname>-dev \
  --name test-vm \
  --query publicIps -o tsv
ssh azureuser@<ip>
```

(run these twice)

```
for i in {1..200}; do curl http://10.241.0.5 & done
for i in {1..200}; do curl http://10.241.0.6 & done
```

check for events and new pods (may take upto 30sec)

```
kubectl events --for hpa/keda-hpa-app-one-request-scaler -w
kubectl events --for hpa/keda-hpa-app-two-request-scaler -w
kubectl get pod
```

You should now see a pod/node scale up based on the metrics from the nginx instances (rather than the ingress controller covered below). The metrics scrape intervals are 30seconds, if you need it quicker this can be adjusted in the ama-metrics-settings-configmap.yaml. Note there may be quotas that prevent a new node allocation (you can view AKS>Node Pools > Autoscale warnings) and you'd need to request additional quota in the azure portal.

az vm delete -g rg-kedascalerapp-dev -n test-vm --yes --no-wait --force-deletion yes (you'll also need to go delete the disk/IP/NIC/NSG)

### Testing Scaler with Nginx ingress controller

Get the IP below and navigate to /app-one /app-two subpaths

kubectl get service -n app-routing-system nginx -o jsonpath="{.status.loadBalancer.ingress[0].ip}"


(run these twice)

```
for i in {1..200}; do curl http://<IP>/app-one & done
for i in {1..200}; do curl http://<IP>/app-two & done
```

check for events and new pods (may take upto 30sec)
```
kubectl events --for hpa/keda-hpa-app-one-request-scaler -w
kubectl events --for hpa/keda-hpa-app-two-request-scaler -w
kubectl get pod
```


You should now see a pod/node scale up based on the metrics from the nginx instances (rather than the ingress controller covered below). Note there may be quotas that prevent a new node allocation (you can view AKS>Node Pools > Autoscale warnings) and you'd need to request additional quota in the azure portal.

### Metrics

These can be viewed from your Azure monitor instance 'metrics' blade as well as from AKS and your Managed grafana if it was enabled.
There are sample dashboards on the project root [README.md](../README.md) to view the related metrics

### Remove workload and dependencies (not infra)

/scripts/undeploy-azure-workload.sh

### Making changes to the chart / app

You can update the chart / values and update via the script: /scripts/update-azure-workload.sh

## Workload Troubleshooting

### Check autoscaler is running

Check keda can get to the metrics and apply the ScaledObjects
kubectl logs -f -n keda -l app=keda-operator

Check the workload identity is correctly setup:
kubectl describe po $KEDA_POD_ID -n keda

Check events from HPA for errors
kubectl events --for hpa/keda-hpa-app-one-request-scaler -w
kubectl events --for hpa/keda-hpa-app-two-request-scaler -w

### Check ingress controller

kubectl get ingress (should show ingress with a host/ip)
kubectl get pod -n ingress-nginx (should show an ingress controller as ready)
kubectl logs -l app.kubernetes.io/component=controller -n ingress-nginx (check controller logs for errors)

### Internal load balancer IPs (when using privateNetwork option)

There may be permissions missing fron NetworkContributor for the aks managed identity

kubectl get service
confirm they have external IPs

kubectl run test-connectivity --rm -it --image=busybox -- sh
> wget -O- http://<pod-ip>:<pod-port> (to test different pods)

## Azure Environment Troubleshooting

### Check Azure monitor prometheus scraping

Azure monitor agent acts as the prometheus scraper, you can check it is running correctly here:
kubectl get ds ama-metrics-node --namespace=kube-system

Browse to the Azure Monitor instance in Azure Portal and query Metrics using the values-azure.yaml scaleQuery to verify data flowing
